{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions used in the regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Squared Loss (a.k.a. Mean Squared Error; MSE)\n",
    "\n",
    "$$ MSE = \\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It calculates the squared difference between the actual value and predicted value (order doesn't matter).\n",
    "\n",
    "1. If the difference is bigger, it gives exponentially bigger lossses: it values the outlier more.\n",
    "2. Most used in regression loss for it is good to be used in least squared method / gradient descent problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Absolute Loss (a.k.a. Mean Absolute Error; MAE)\n",
    "\n",
    "$$ MAE = \\frac{1}{N}\\sum_{i=1}^{N}|y_i - \\hat{y}_i|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It calculates the absolute difference between the actual value and predicted value (order doesn't matter).\n",
    "\n",
    "1. It doesn't count the outlier compared to the MSE, so if the sample is splitted into two, it tends to fit into more major sample group.\n",
    "2. When it comes to the least squared method / gradient descent, absolute sign gives it hard to compute / converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_loss(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Huber Loss\n",
    "\n",
    "$$ \\text{Huber}(y_i, \\hat{y}_i) = \\begin{cases} \n",
    "    \\frac{1}{2}(y_i - \\hat{y}_i)^2 & |y_i - \\hat{y}_i| \\leq \\delta \\\\\n",
    "    \\delta |y_i - \\hat{y}_i| - \\frac{1}{2}\\delta ^2 & \\text{otherwise}\n",
    "\\end{cases}  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It merges the MAE and MSE.\n",
    "1. It uses squared error for the small error, but it uses absolute error for the large error, so it works similar to the MAE.\n",
    "2. It is useful to decrease the sensitivity to the outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    diff = y_true - y_pred\n",
    "    loss = np.where(diff <= delta, 0.5 * diff**2, delta * (diff - 0.5 * delta))\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions used in the classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Binary Cross-Entropy Loss\n",
    "\n",
    "$$\n",
    "\\text{Cross-Entropy Loss} = -\\sum_{i=1}^{N}(y_i log(\\hat{y}_i) + (1-y_i)log(1-\\hat{y}_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_i$: real y label (0, 1)\n",
    "\n",
    "$\\hat{y}_i$: predicted percentages (0 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log(x) from 0 to 1 returns negative value, so the total loss is multiplied by -1.\n",
    "\n",
    "It calculates how close the prediction from the actual value. by multiplying $y_{true}$ and $1-y_{true}$ in front of each log term, it can only focus on the targeted y class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon) # 예측값을 [epsilon, 1-epsilon 범위로 제한] 함으로 exponential한 값 막기\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1-y_true) * np.log(1-y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Multicategorical Cross-Entropy Loss\n",
    "\n",
    "$$\n",
    "\\text{Multi Cross-Entropy Loss} = -\\sum _{i=1}^{C}y_i log(\\hat{y}_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It calculates the cross entropy, but with many classes all together. we use one-hot encoded y, which gives 1 for answer class and others for 0, so it can still stick into the targeted y class and get the summation of the log loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C$: number of classes\n",
    "$y_i$: one-hot encoded: (answer class:1, others:0)\n",
    "$\\hat{y}_i$: predicted percentages for each classes\n",
    "\n",
    "ex) [0.1, 0.8, 0.1] and 2nd class is right => lss = -log(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_crossentropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Killback-Leibler Divergence (KL Divergence)\n",
    "\n",
    "$$\n",
    "D_{KL}(P||Q) = \\sum_{i=1}^{N}P(x_i)log\\frac{P(x_i)}{Q(x_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(x_i)$: actual (true) distribution's probability\n",
    "\n",
    "$Q(x_i)$: predicted distribution's probability\n",
    "\n",
    "$D_{KL}(P||Q)$: KL Divergence between distribution P and distribution Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is used to Variational Auto Encoder (VAE), of Baysian Network.\n",
    "\n",
    "it is an asymmetric loss function to measure the difference between two distributions. So we need to clarify which distribution is used for the standard (true) one.\n",
    "\n",
    "1. It is asymmetirc. It always have 0 or positive number, and if P and Q are the same, it is to 0. If they are different, it has the positive value.\n",
    "2. Information loss: KL divergence shows the loss of information from true distribution to another distribution. To illustrate, it shows how many information will be lost if we use alternative distribution rather than true distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(P, Q):\n",
    "    epsilon = 1e-10\n",
    "    P = np.clip(P, epsilon, 1)\n",
    "    Q = np.clip(Q, epsilon, 1)\n",
    "    \n",
    "    return np.sum(P * np.log(P / Q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
