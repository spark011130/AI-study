{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is different than linear regression?\n",
    "Q2. How to tune the parameters?\n",
    "Q3. Why do we need hidden layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./files/DNN/DNN_example.png\" width=\"400\" height=\"250\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each element formulation\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_1 &= x_1w_1 + x_2w_2 + b_1 \\\\\n",
    "z_2 &= x_1w_3 + x_2w_4 + b_2 \\\\\n",
    "z_3 &= h_1w_5 + h_2w_6 + b_3 \\\\\n",
    "z_4 &= h_1w_7 + h_2w_8 + b_4 \\\\\n",
    "\n",
    "h_1 &= \\sigma (z_1) \\\\\n",
    "h_2 &= \\sigma (z_2) \\\\\n",
    "\n",
    "o_1 &= \\sigma (z_3) \\\\\n",
    "o_2 &= \\sigma (z_4) \\\\\n",
    "\n",
    "E_{o1} &= \\frac{1}{2}(y_1 - o_1)^2 \\\\\n",
    "E_{o2} &= \\frac{1}{2}(y_2 - o_2)^2 \\\\\n",
    "E_{total} &= E_{o1} + E_{o2} \\\\\n",
    "\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the gradient of $w_5$:\n",
    "$$ \\begin{align*}\n",
    "\\frac{\\partial E_{total}}{\\partial w_5} &= \\frac{\\partial E_{total}}{\\partial o_1} \\cdot \\frac{\\partial o_1}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_5} \\\\\n",
    "&= \\frac{\\partial (E_{o1} + E_{o2})}{\\partial o_1} \\cdot \\frac{\\partial o_1}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_5} \\\\\n",
    "& = -(y_1 - o_1) \\cdot \\sigma '(z_3) \\cdot h_1\n",
    "\\end{align*} $$\n",
    "Using optimizer, we can learn the $w_5$ into more optimized value.\n",
    "\n",
    "Fitting the gradient of $b_3$, is very similar to $w_5$:\n",
    "$$ \\begin{align*}\n",
    "\\frac{\\partial E_{total}}{\\partial w_5} &= \\frac{\\partial E_{total}}{\\partial o_1} \\cdot \\frac{\\partial o_1}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial b_3} \\\\\n",
    "&= \\frac{\\partial (E_{o1} + E_{o2})}{\\partial o_1} \\cdot \\frac{\\partial o_1}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial b_3} \\\\\n",
    "& = -(y_1 - o_1) \\cdot \\sigma '(z_3) \\cdot 1\n",
    "\\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this method, we can fit $w_5, w_6, w_7, w_8, b_3, b_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla w_5 &= \\frac{\\partial E_{total}}{\\partial o_1} \\cdot \\frac{\\partial o_1}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_5} \\\\\n",
    "&= -(y_1 - o_1) \\cdot \\sigma '(z_3) \\cdot h_1 \\\\\n",
    "\n",
    "\\nabla w_7 &= \\frac{\\partial E_{total}}{\\partial o_2} \\cdot \\frac{\\partial o_2}{\\partial z_4} \\cdot \\frac{\\partial z_4}{\\partial w_7} \\\\\n",
    "& = -(y_2 - o_2) \\cdot \\sigma '(z_4) \\cdot h_1 \\\\\n",
    "\n",
    "\\nabla w_6 &= \\frac{\\partial E_{total}}{\\partial o_1} \\cdot \\frac{\\partial o_1}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_6}\\\\\n",
    "&= -(y_1 - o_1) \\cdot \\sigma '(z_3) \\cdot h_2 \\\\\n",
    "\n",
    "\\nabla w_8 &= \\frac{\\partial E_{total}}{\\partial o_2} \\cdot \\frac{\\partial o_2}{\\partial z_4} \\cdot \\frac{\\partial z_4}{\\partial w_8} \\\\\n",
    "& = -(y_2 - o_2) \\cdot \\sigma '(z_4) \\cdot h_2 \\\\\n",
    "\n",
    "\\nabla b_3 &= \\frac{\\partial E_{total}}{\\partial o_1} \\cdot \\frac{\\partial o_1}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial b_3} \\\\\n",
    "&= -(y_1 - o_1) \\cdot \\sigma '(z_3) \\cdot 1 \\\\\n",
    "\n",
    "\\nabla b_4 &= \\frac{\\partial E_{total}}{\\partial o_2} \\cdot \\frac{\\partial o_2}{\\partial z_4} \\cdot \\frac{\\partial z_4}{\\partial b_4} \\\\\n",
    "& = -(y_2 - o_2) \\cdot \\sigma '(z_4) \\cdot 1 \\\\ \n",
    "\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Backpropagation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going one more step forward, now getting the gradients of $w_1, w_2, w_3, w_4, b_1, b_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to calculate the derivative of $h_1, h_2$.\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial E_{total}}{\\partial h_1} &= \\frac{\\partial E_{o1}}{\\partial h_1} + \\frac{\\partial E_{o2}}{\\partial h_1} \\\\\n",
    "    \n",
    "    &=  \\frac{\\partial E_{o1}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial h_1}  + \\frac{\\partial E_{o2}}{\\partial z_4} \\cdot \\frac{\\partial z_4}{\\partial h_1}\\\\\n",
    "    \n",
    "    &= \\frac{\\partial E_{o1}}{\\partial o_1} \\cdot \\frac{\\partial o_1}{\\partial z_3}  \\cdot \\frac{\\partial z_3}{\\partial h_1} + \\frac{\\partial E_{o2}}{\\partial o_2} \\cdot \\frac{\\partial o_2}{\\partial z_4}  \\cdot \\frac{\\partial z_4}{\\partial h_1}  \\\\\n",
    "\n",
    "    & = -(y_1 - o_1) \\cdot \\sigma '(z_3) \\cdot w_5 + -(y_2 - o_2) \\cdot \\sigma ' (z_4) \\cdot w_7 \\\\\n",
    "\n",
    "    \\frac{\\partial E_{total}}{\\partial h_2} &= \\frac{\\partial E_{o1}}{\\partial h_2} + \\frac{\\partial E_{o2}}{\\partial h_2} \\\\\n",
    "\n",
    "    &= \\frac{\\partial E_{o1}}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial h_2} + \\frac{\\partial E_{o2}}{\\partial z_4} \\cdot \\frac{\\partial z_4}{\\partial h_2} \\\\\n",
    "\n",
    "    &= \\frac{\\partial E_{o1}}{\\partial o_1} \\cdot \\frac{\\partial o_1}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial h_2} + \\frac{\\partial E_{o2}}{\\partial o_2} \\cdot \\frac{\\partial o_2}{\\partial z_4} \\cdot \\frac{\\partial z_4}{\\partial h_2} \\\\\n",
    "\n",
    "    &= -(y_1 - o_1) \\cdot \\sigma '(z_3) \\cdot w_6 + -(y_2 - o_2) \\cdot \\sigma '(z_4) \\cdot w_8\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this, the final step is here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial E_{total}}{\\partial w_1} &= \\frac{\\partial E_{total}}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_1} \\\\\n",
    "    &= \\frac{\\partial E_{total}}{\\partial h_1} \\cdot \\sigma ' (z_1) \\cdot x_1 \\\\\n",
    "\n",
    "    \\frac{\\partial E_{total}}{\\partial w_2} &= \\frac{\\partial E_{total}}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_2} \\\\\n",
    "    &= \\frac{\\partial E_{total}}{\\partial h_1} \\cdot \\sigma ' (z_1) \\cdot x_2 \\\\\n",
    "\n",
    "    \\frac{\\partial E_{total}}{\\partial w_3} &= \\frac{\\partial E_{total}}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_3} \\\\\n",
    "    &= \\frac{\\partial E_{total}}{\\partial h_2} \\cdot \\sigma ' (z_2) \\cdot x_1 \\\\\n",
    "\n",
    "    \\frac{\\partial E_{total}}{\\partial w_4} &= \\frac{\\partial E_{total}}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_4} \\\\\n",
    "    &= \\frac{\\partial E_{total}}{\\partial h_2} \\cdot \\sigma ' (z_2) \\cdot x_2 \\\\\n",
    "\n",
    "    \\frac{\\partial E_{total}}{\\partial b_1} &= \\frac{\\partial E_{total}}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial b_1} \\\\\n",
    "    &= \\frac{\\partial E_{total}}{\\partial h_1} \\cdot \\sigma ' (z_1) \\cdot 1 \\\\\n",
    "\n",
    "    \\frac{\\partial E_{total}}{\\partial b_2} &= \\frac{\\partial E_{total}}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial b_2} \\\\\n",
    "    &= \\frac{\\partial E_{total}}{\\partial h_1} \\cdot \\sigma ' (z_1) \\cdot 1 \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalization is pretty simpler:\n",
    "\n",
    "$$\n",
    "\\begin {align*}\n",
    "\\frac{\\partial L}{\\partial w_N} &= \\frac{\\partial L}{\\partial a_N} \\cdot \\frac{\\partial a_N}{\\partial z_N}  \\cdot \\frac{\\partial z_N}{\\partial w_N} \\\\\n",
    "\\frac{\\partial L}{\\partial b_N} &= \\frac{\\partial L}{\\partial a_N} \\cdot \\frac{\\partial a_N}{\\partial z_N} \\cdot \n",
    "(\\frac{\\partial z_N}{\\partial b_N} =1) \\\\\n",
    "&= \\frac{\\partial L}{\\partial a_N} \\cdot \\frac{\\partial a_N}{\\partial z_N}\n",
    "\\end {align*}\n",
    "$$\n",
    "\n",
    "Break then into each partial derivative pieces, \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a_N}: \\text{loss in output layer.} \\\\ \\\\\n",
    "\\frac{\\partial a_N}{\\partial z_N}: \\text{partial derivative of activation function.} \\\\ \\\\\n",
    "\\frac{\\partial z_N}{\\partial w_N / b_N}: \\text{partial derivative of weights, or bias.} \\\\ \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DNN:\n",
    "    def __init__(self, input_size, hidden_layers, output_size, learning_rate = 0.01):\n",
    "        '''\n",
    "        Initiate the terms in the DNN according to the parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        input_size (int): size of the input\n",
    "        hidden_layers (list): number of neurons of each layer\n",
    "        output_size (int): size of the output\n",
    "        learning_rate (int): learning rate\n",
    "        \n",
    "        Returns:\n",
    "        self.input_size (int): input size ([1 x input_size])\n",
    "        self.hidden_layers (int): hidden layer sizes list\n",
    "        self.output_size (int): output size\n",
    "        self.learning_rate (int): learning rate\n",
    "        self.weights (list): [#layer x #prev_layer x #current_layer] randomized weights\n",
    "        self.biases (list): [#layer x #current_layer x 1] randomized biases\n",
    "        '''\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        #initialize weights and biases for the hidden layer + output layer.\n",
    "        prev_layer_size = self.input_size\n",
    "        for hidden_layer_size in (self.hidden_layers + [output_size]):\n",
    "            weight = np.random.randn(prev_layer_size, hidden_layer_size) * 0.01 # std_norm_dist * 0.01\n",
    "            bias = np.zeros((1, hidden_layer_size))\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "            prev_layer_size = hidden_layer_size\n",
    "            \n",
    "    def forward_propagation(self, X):\n",
    "        '''\n",
    "        Forward propagation to get initial guesses\n",
    "        \n",
    "        Parameters:\n",
    "        X (list): input list\n",
    "        \n",
    "        Returns:\n",
    "        self.a (list): X + every neuron's value after the activation function.\n",
    "        self.z (list): X + every neuron's value before the activation function.\n",
    "        '''\n",
    "        self.a = [X]\n",
    "        self.z = []\n",
    "        for i in range(self.hidden_layers + 1):\n",
    "            z = np.dot(self.a[i], self.weights[i]) + self.bias[i]\n",
    "            self.z.append(z)\n",
    "            a = self.sigmoid(z)\n",
    "            self.a.append(a)\n",
    "        return self.a[-1] # Return output layer activations\n",
    "    \n",
    "    def backward_propagation(self, X, y):\n",
    "        '''\n",
    "        Backward propagation to get gradients for every weights, and biases.\n",
    "        \n",
    "        Parameters:\n",
    "        X (list): batched list (batch_size x input_size).\n",
    "        Example of the X\n",
    "        X = np.array([[0.5, 0.1, -0.2],  # First sample, 3 features\n",
    "              [0.2, 0.4, 0.1]])  # Second sample, 3 features\n",
    "        Total of 2 batch size\n",
    "        where sample: one lump of data, features: ex) a person's height, or weight, or etc.\n",
    "        \n",
    "        y (list): true Y for each sample (batch_size y x output_size).\n",
    "        \n",
    "        Returns: \n",
    "        np.array(dW[::-1]) (list): gradient of W \n",
    "        np.array(db[::-1]) (list): gradient of b\n",
    "        '''    \n",
    "        # Calculate output Layer error\n",
    "        self.batch_size = X.shape[0] # number of batches\n",
    "        \n",
    "        y_pred = self.forward(X)\n",
    "\n",
    "        # Loss function derivative\n",
    "        loss_derivative = self.loss_derivative(y_pred, y) # = d L / d a_N\n",
    "        \n",
    "        # Backpropagation through layers\n",
    "        dA = loss_derivative * self.sigmoid_derivative(self.a[-1]) # Output Layer; * d a_N / d z_N\n",
    "        dZ = dA # For output layer, dZ = dA because sigmoid derivative is applied\n",
    "        \n",
    "        # Update weights and biases for the output layer\n",
    "        dW = np.dot(self.a[-2].T, dZ) # * d z_N / d w_N\n",
    "        db = np.sum(dZ, axis=0, keepdims=True) # dZ itself\n",
    "        \n",
    "        # Propagate the gradient back through the hidden layers\n",
    "        dA = np.dot(dZ, self.weights[-1].T)  # Error propagated back to previous layer\n",
    "        for i in range(len(self.weights) - 2, -1, -1):  # Loop through hidden layers\n",
    "            dZ = dA * self.sigmoid_derivative(self.a[i + 1])\n",
    "            dW.append(np.dot(self.a[i].T, dZ)) # * d z_N / d w_N\n",
    "            db.append(np.sum(dZ, axis=0, keepdims=True)) # dZ itself\n",
    "            dA = np.dot(dZ, self.weights[i].T)\n",
    "        return np.array(dW[::-1]), np.array(db[::-1])\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        # Mean Squared Error loss function\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def loss_derivative(self, y_pred, y_true):\n",
    "        # Mean Squared Error loss' derivative function\n",
    "        return 2 * (y_pred - y_true) / self.batch_size\n",
    "        \n",
    "    def apply_gradient(self, dW, db):\n",
    "        # Update gradients and biases using the gradient (gradient descent)\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * dW[i]\n",
    "            self.biases[i] -= self.learning_rate * db[i]\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, print_rate = 10):\n",
    "        for epoch in tqdm(epochs):\n",
    "            self.backward_propagation(X, y)\n",
    "            if epoch % print_rate == 0:\n",
    "                y_pred = self.forward(X)\n",
    "                loss = self.compute_loss(y_pred, y)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "                \n",
    "    def train_mini_batch(self, X, y, batch_size=32, epoches=1000, print_rate = 10):\n",
    "        m = X.shape[0]\n",
    "        for epoch in range(epoches):\n",
    "            indices = np.random.permutation(m)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            for i in range(0, m, batch_size):  # Repetition in batch_size measure\n",
    "                # Extraction\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                # 현재 배치에 대해 역전파 수행\n",
    "                dW, db = self.backward(X_batch, y_batch)\n",
    "                \n",
    "                # 가중치와 편향 업데이트\n",
    "                self.apply_gradient(dW, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "A = np.array([1, 2, 3, 4, 5])\n",
    "print(A.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
