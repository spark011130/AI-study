{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN (Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN is a type of network that is great for processing sequences of data. It memorizes the past inputs, which means it shares the past weight to process the current input. The backpropagation method is done through time; a.k.a BPTT (Back Propagation Through Time).\n",
    "\n",
    "It is used in natural language processing (NLP), time series prediction, and speech/audio processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each Element Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./files/RNN/RNN_notes.jpeg\" width=\"1000\" height=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "Compute all of the forward propagation according to the time. We can get hidden layers' value, and losses from it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Losses and Hs from forward propagation, we can BPTT (Back Propagation Through Time) from t=T to 1 (in practice, it is T-1 to 0).\n",
    "\n",
    "Every gradient in each and every time is added together, and the sum of the gradients will be used to tune the hyperparameters.\n",
    "\n",
    "First, we are going to get the gradients of $h_t, b_y, W_{hy}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let Loss $L = \\sum(y_t - y_t^{true})^2$, where $y_t$ is a output value after the softmax, and $y_t^{true}$ is one-hot encoded true value.\n",
    "$$\n",
    "L = \\sum_{t=0}^{T-1}(y_t - y_t^{true})^2 \\\\\n",
    "\\frac{\\partial L}{\\partial y_t} = 2\\sum_{t=0}^{T-1}(y_{t} - y_{t}^{true})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that\n",
    "$$ \n",
    "Z_t = w_x x_t + w_h h_{t-1} + b_h \\\\\n",
    "h_t = tanh(Z_t) \\\\\n",
    "y_t = w_y h_t + b_y \\\\\n",
    "L_t = (y_t - y_t^{true})^2 \\\\\n",
    "$$ \n",
    "We need to tune five hyperparameters: $b_y, w_y, w_h, b_h,$ and $w_x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate gradients of $b_y$ and $w_y$.\n",
    "$$\n",
    "\\begin {align*}\n",
    "\\nabla b_y &= \\frac{\\partial L}{\\partial b_y} = \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial b_y} \\\\\n",
    "&= 2\\sum(y_t - y_t^{true}) \\cdot 1 \\\\\n",
    "\\nabla w_y &= \\frac{\\partial L}{\\partial w_y} = \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial w_y} \\\\\n",
    "&=2\\sum(y_t - y_t^{true}) \\cdot h_t \\\\\n",
    "\\end {align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to calculate $b_h, w_h$ and $w_x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using:\n",
    "$$ \\begin{align*}\n",
    "\\nabla Z_t &= \\frac{\\partial L}{\\partial Z_t} = \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial Z_t}\\\\\n",
    "&= 2\\sum(y_t - y_t^{true}) \\cdot w_y \\cdot (1-tanh^2(Z_t))\n",
    "\\end{align*} $$\n",
    "We can calculate all of them:\n",
    "$$ \\begin{align*}\n",
    "\n",
    "\\nabla b_h &= \\frac{\\partial L}{\\partial b_h} = \\frac{\\partial L}{\\partial Z_t} \\cdot \\frac{\\partial Z_t}{\\partial b_h} \\\\\n",
    "&= \\nabla Z_t \\cdot 1 \\\\\n",
    "&= 2\\sum(y_t - y_t^{true}) \\cdot w_y \\cdot (1-tanh^2(Z_t))\\\\\n",
    "\n",
    "\\nabla w_h &= \\frac{\\partial L}{\\partial w_h} = \\frac{\\partial L}{\\partial Z_t} \\cdot \\frac{\\partial Z_t}{\\partial w_h} \\\\\n",
    "&= \\nabla Z_t \\cdot h_{t-1} \\\\\n",
    "&= 2\\sum(y_t - y_t^{true}) \\cdot w_y \\cdot (1-tanh^2(Z_t)) \\cdot h_{t-1}\\\\\n",
    "\n",
    "\\nabla w_x &= \\frac{\\partial L}{\\partial w_x} = \\frac{\\partial L}{\\partial Z_t} \\cdot \\frac{\\partial Z_t}{\\partial w_x} \\\\\n",
    "&= \\nabla Z_t \\cdot X_t \\\\\n",
    "&= 2\\sum(y_t - y_t^{true}) \\cdot w_y \\cdot (1-tanh^2(Z_t)) \\cdot X_t \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't use the batch training, for the samples are not independent, but has a sequential relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 161>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Example data generator\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_data\u001b[39m(num_samples, input_size, output_size):\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# generate X (from std norm dist)\u001b[39;00m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mDNN.__init__\u001b[0;34m(self, input_size, hidden_size, output_size, learning_rate)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m learning_rate\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#Initialize all the weights and biases.\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWxh \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWhh \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(hidden_size, hidden_size) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWhy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(output_size, hidden_size) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:1306\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randn\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:1467\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.standard_normal\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_common.pyx:657\u001b[0m, in \u001b[0;36mnumpy.random._common.cont\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate = 0.01):\n",
    "        '''\n",
    "        Initiate the terms in the RNN according to the parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        input_size (int): size of the input\n",
    "        hidden_size (size): neurons in the hidden layer\n",
    "        output_size (int): size of the output\n",
    "        learning_rate (int): learning rate\n",
    "        \n",
    "        Returns:\n",
    "        self.input_size (int): input size\n",
    "        self.hidden_size (int): hidden layer sizes list\n",
    "        self.output_size (int): output size\n",
    "        self.learning_rate (int): learning rate\n",
    "        self.Wx (np.array): [#hidden size x #input size] x_t to h_t (Z_th) weights\n",
    "        self.Wh (np.array): [#hidden size x #hidden size] h_t-1 to h_t weights\n",
    "        self.Wy (np.array): [#output size x #hidden size] h_t to y_t (Z_ty) weights\n",
    "        \n",
    "        self.by (np.array): [#hidden size x 1] h_t to y_t (Z_ty) biases\n",
    "        self.bh (np.array): [#output size x 1] x_t to h_t (Z_th) biases\n",
    "        '''\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        #Initialize all the weights and biases.\n",
    "        self.Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        \n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "            \n",
    "    def forward_propagation(self, X):\n",
    "        '''\n",
    "        Forward propagation, X is given as a sequential data.\n",
    "        \n",
    "        parameters:\n",
    "        X (list): [T, input size]\n",
    "\n",
    "        returns:\n",
    "        retY (list): predicted ys\n",
    "        retH (list): calculated Hs (not include 0 matrix)\n",
    "        '''\n",
    "        seq_len, _ = X.shape\n",
    "        retY = []\n",
    "        retH = []\n",
    "        #initiate parameters\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            z = np.dot(self.Wx, X.reshape(-1, 1)) + np.dot(self.Wh, h) + self.bh.reshape(-1, 1)\n",
    "            h = np.tanh(z)\n",
    "            y = np.dot(self.Wy, h) + self.by.reshape(-1, 1)\n",
    "            retH.append(h)\n",
    "            retY.append(y)\n",
    "\n",
    "        return retY, retH\n",
    "    \n",
    "    def backward_propagation(self, X, y):\n",
    "        '''\n",
    "        Backward propagation to get gradients for every weights, and biases.\n",
    "        '''\n",
    "        dWx, dWh, dWy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        \n",
    "        seq_len, _ = X.shape\n",
    "        h = np.zeros((seq_len, self.hidden_size, 1)) # save all time step's hidden states\n",
    "        \n",
    "        # Forward Pass\n",
    "        y_pred, h = self.forward_propagation(X)\n",
    "        \n",
    "        for t in reversed(range(seq_len)):\n",
    "            dy = self.loss_derivative(y_pred[t], y[t])\n",
    "\n",
    "            dby += dy\n",
    "            dWy += np.dot(dy, h[t]) \n",
    "\n",
    "\n",
    "\n",
    "        # Backward Pass (BPTT)\n",
    "        dh_next = np.zeros_like(h[0]) #initiate the hidden state gradient for next time step\n",
    "        for t in reversed(range(seq_len)):\n",
    "            dy = self.loss_derivative(y_pred[t], y[t])\n",
    "            \n",
    "            dWy += np.dot(dy, h[t].T)\n",
    "            dby += dy\n",
    "            \n",
    "            dh = np.dot(self.Wy.T, dy) + dh_next\n",
    "            dz = dh * (1 - h[t] ** 2) # tanh derivative\n",
    "            \n",
    "            dWx += np.dot(dz, X[t].shape(-1, 1))\n",
    "            dWh += np.dot(dz, h[t - 1].T if t > 0 else np.zeros_like(h[t]))\n",
    "            dbh += dz\n",
    "            \n",
    "            dh_next = np.dot(self.Wh.T, dz)\n",
    "            \n",
    "        return dWx, dWh, dWy, dbh, dby\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        # Mean Squared Error loss function\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def loss_derivative(self, y_pred, y_true):\n",
    "        # Mean Squared Error loss' derivative function\n",
    "        return 2 * (y_pred - y_true) # divided by batch_size; in this case, it is 1.\n",
    "        \n",
    "    def optimizer(self, dWxh, dWhh, dWhy, dbh, dby):\n",
    "        # Update gradients and biases using the gradient (gradient descent)\n",
    "        self.Wxh -= self.learning_rate * dWxh\n",
    "        self.Whh -= self.learning_rate * dWhh\n",
    "        self.Why -= self.learning_rate * dWhy\n",
    "        self.bh -= self.learning_rate * dbh\n",
    "        self.by -= self.learning_rate * dby\n",
    "                \n",
    "    def train_mini_batch(self, X_train, y_train, batch_size=32, epoches=1000, print_rate = 10):\n",
    "        m = X.shape[0]\n",
    "        for epoch in tqdm(range(epoches)):\n",
    "            indices = np.random.permutation(m)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train[indices]\n",
    "            \n",
    "            for i in range(0, m, batch_size):  # Repetition in batch_size measure\n",
    "                # Extraction\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                dWxh, dWhh, dWhy, dbh, dby = self.backward_propagation(X_batch, y_batch)\n",
    "                self.optimizer(dWxh, dWhh, dWhy, dbh, dby)\n",
    "                \n",
    "            if epoch % 10 == 0:\n",
    "                y_pred = self.forward_propagation(X_train)[0]\n",
    "                # training loss calculation\n",
    "                loss = self.compute_loss(y_pred, y_train)\n",
    "                print(f'{epoch}-th epoch MSE loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
