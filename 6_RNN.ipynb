{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN (Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN is a type of network that is great for processing sequences of data. It memorizes the past inputs, which means it shares the past weight to process the current input. The backpropagation method is done through time; a.k.a BPTT (Back Propagation Through Time).\n",
    "\n",
    "It is used in natural language processing (NLP), time series prediction, and speech/audio processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each Element Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./files/RNN/RNN_example.png\" width=\"400\" height=\"250\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./files/RNN/RNN_memo.png\" width=\"600\" height=\"250\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a **Input Layer**, **Hidden State**, and **Output Layer**. \n",
    "\n",
    "Hidden state is calculated as:\n",
    "$$\n",
    "z_{th} = W_{xh}x_t + W_{hh}h_{t-1} + b_h \\\\\n",
    "h_t = tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h) = tanh(z_{th})\n",
    "$$\n",
    "Where $h_t$ is hidden state in time t, $W_{xh}$ is input => current hidden state weight, and $W_{hh}$ is past hidden state => current hidden state weight.\n",
    "\n",
    "Output Layer is calculated as:\n",
    "$$\n",
    "z_{ty} = W_{hy}h_t + b_y \\\\ \n",
    "y_t = softmax(W_{hy}h_t + b_y) = \\sigma (W_{hy}h_t + b_y) = \\sigma (z_{ty})\\\\\n",
    "\n",
    "$$\n",
    "Where $W_{hy}$ is current hidden state => logit weight, and $b_y$ is its bias.\n",
    "\n",
    "As you can see, the input->hidden state weight/bias and hidden state->output weight/bias are not depending on the time and it is shared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the input sequence, get the output predictions. And by the error calculation, we can optimize the weight using back propagation.\n",
    "\n",
    "First, we are going to get the gradients of $h_t, b_y, W_{hy}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let Loss $L = \\sum(y_t - y_t^{true})^2$, where $y_t$ is a output value after the softmax, and $y_t^{true}$ is one-hot encoded true value.\n",
    "$$\n",
    "L = \\sum(y_t - y_t^{true})^2 \\\\\n",
    "\\frac{\\partial L}{\\partial y_t} = 2(y_{t} - y_{t}^{true})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\n",
    "$$ \n",
    "y_t = \\sigma (W_{hy}h_t + b_y) \\\\\n",
    "z_{ty} = W_{hy}h_t + b_y \\\\ \n",
    "\\frac{\\partial y_t}{\\partial z_{ty}} = \\sigma '(z_{ty})\n",
    "$$\n",
    "If so,\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial h_t} &= \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial z_{ty}} \\cdot \\frac{\\partial z_{ty}}{\\partial h_t} \\\\\n",
    "&= 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot W_{hy} \\\\ \n",
    "\n",
    "\\frac{\\partial L}{\\partial b_y} &= \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial z_{ty}} \\cdot \\frac{\\partial z_{ty}}{\\partial b_y}\\\\\n",
    "&= 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot 1 \\\\ \n",
    "\n",
    "\\frac{\\partial L}{\\partial W_{hy}} &= \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial z_{ty}} \\cdot \\frac{\\partial z_{ty}}{\\partial W_{hy}}\\\\\n",
    "&= 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot h_t \\\\ \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to calculate $W_{xh}, W_{hh}, b_h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{align*}\n",
    "\\frac{\\partial L}{\\partial W_{xh}} &= \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial z_{ty}} \\cdot \\frac{\\partial z_{ty}}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial z_{th}} \\cdot \\frac{\\partial z_{th}}{\\partial W_{xh}} \\\\\n",
    "&= 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot W_{hy} \\cdot h_t \\cdot x_t \\\\\n",
    "\n",
    "\\frac{\\partial L}{\\partial W_{hh}} &= \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial z_{ty}} \\cdot \\frac{\\partial z_{ty}}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial z_{th}} \\cdot \\frac{\\partial z_{th}}{\\partial W_{hh}} \\\\\n",
    "&= 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot W_{hy} \\cdot h_t \\cdot h_{t-1} \\\\\n",
    "\n",
    "\\frac{\\partial L}{\\partial b_h} &= \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial z_{ty}} \\cdot \\frac{\\partial z_{ty}}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial z_{th}} \\cdot \\frac{\\partial z_{th}}{\\partial b_h} \\\\\n",
    "&= 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot W_{hy} \\cdot h_t \\cdot 1\n",
    "\n",
    "\\end{align*} $$\n",
    "We could also use this for compact computation:\n",
    "$$ \\begin{align*}\n",
    "\\frac{\\partial L}{\\partial h_t} &= \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial z_{ty}} \\cdot \\frac{\\partial z_{ty}}{\\partial h_t} \\\\\n",
    "&=2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot W_{hy}\n",
    "\\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization\n",
    "It has already been generalized. Just to write down all the gradients together:\n",
    "$$\n",
    "\\begin {align*}\n",
    "\n",
    "\\nabla b_y &= \\frac{\\partial L}{\\partial b_y} = 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot 1 \\\\ \n",
    "\n",
    "\\nabla W_{hy} &= \\frac{\\partial L}{\\partial W_{hy}} = 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot h_t \\\\ \n",
    "\n",
    "\\nabla W_{xh} &= \\frac{\\partial L}{\\partial W_{xh}} = 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot W_{hy} \\cdot h_t \\cdot x_t \\\\\n",
    "\n",
    "\\nabla W_{hh} &= \\frac{\\partial L}{\\partial W_{hh}} = 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot W_{hy} \\cdot h_t \\cdot h_{t-1} \\\\\n",
    "\n",
    "\\nabla b_h &= \\frac{\\partial L}{\\partial b_h} = 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot W_{hy} \\cdot h_t \\cdot 1 \\\\\n",
    "\\end {align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the sequential $h_t$, we can train the RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't use the batch training, for the samples are not independent, but has a sequential relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate = 0.01):\n",
    "        '''\n",
    "        Initiate the terms in the RNN according to the parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        input_size (int): size of the input\n",
    "        hidden_size (size): neurons in the hidden layer\n",
    "        output_size (int): size of the output\n",
    "        learning_rate (int): learning rate\n",
    "        \n",
    "        Returns:\n",
    "        self.input_size (int): input size ([1 x input_size])\n",
    "        self.hidden_size (int): hidden layer sizes list\n",
    "        self.output_size (int): output size\n",
    "        self.learning_rate (int): learning rate\n",
    "        self.Wxh (np.array): [#hidden size x #input size] x_t to h_t (z_th) weights\n",
    "        self.Whh (np.array): [#hidden size x #hidden size] h_t-1 to h_t weights\n",
    "        self.Why (np.array): [#output size x #hidden size] h_t to y_t (z_ty) weights\n",
    "        \n",
    "        self.by (np.array): [#hidden size x 1] h_t to y_t (z_ty) biases\n",
    "        self.bh (np.array): [#output size x 1] x_t to h_t (z_th) biases\n",
    "        '''\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        #Initialize all the weights and biases.\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Why = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        \n",
    "        self.by = np.zeros((hidden_size, 1))\n",
    "        self.bh = np.zeros((output_size, 1))\n",
    "            \n",
    "    def forward_propagation(self, X):\n",
    "        '''\n",
    "        Forward propagation to get initial guesses\n",
    "        \n",
    "        Parameters:\n",
    "        X (list): input list\n",
    "        \n",
    "        Returns:\n",
    "        self.a (list): X + every neuron's value after the activation function.\n",
    "        self.z (list): X + every neuron's value before the activation function.\n",
    "        '''\n",
    "        self.a = [X]\n",
    "        self.z = []\n",
    "        for i in range(len(self.weights)):\n",
    "            z = np.dot(self.a[i], self.weights[i]) + self.biases[i]\n",
    "            self.z.append(z)\n",
    "            a = self.sigmoid(z)\n",
    "            self.a.append(a)\n",
    "        return self.a[-1] # Return output layer activations\n",
    "    \n",
    "    def backward_propagation(self, X, y):\n",
    "        '''\n",
    "        Backward propagation to get gradients for every weights, and biases.\n",
    "        \n",
    "        Parameters:\n",
    "        X (list): batched list (batch_size x input_size).\n",
    "        Example of the X\n",
    "        X = np.array([[0.5, 0.1, -0.2],  # First sample, 3 features\n",
    "              [0.2, 0.4, 0.1]])  # Second sample, 3 features\n",
    "        Total of 2 batch size\n",
    "        where sample: one lump of data, features: ex) a person's height, or weight, or etc.\n",
    "        \n",
    "        y (list): true Y for each sample (batch_size y x output_size).\n",
    "        \n",
    "        Returns: \n",
    "        np.array(dW[::-1]) (list): gradient of W \n",
    "        np.array(db[::-1]) (list): gradient of b\n",
    "        '''    \n",
    "        # Calculate output Layer error\n",
    "        self.batch_size = X.shape[0] # number of batches\n",
    "        y_pred = self.forward_propagation(X)\n",
    "        # Loss function derivative\n",
    "        loss_derivative = self.loss_derivative(y_pred, y) # = d L / d a_N\n",
    "        \n",
    "        # Backpropagation through layers\n",
    "        dA = loss_derivative * self.sigmoid_derivative(self.a[-1]) # Output Layer; * d a_N / d z_N\n",
    "        dZ = dA # For output layer, dZ = dA because sigmoid derivative is applied\n",
    "        dW = []\n",
    "        db = []\n",
    "        \n",
    "        # Update weights and biases for the output layer\n",
    "        dW.append(np.dot(self.a[-2].T, dZ)) # * d z_N / d w_N\n",
    "        db.append(np.sum(dZ, axis=0, keepdims=True)) # dZ itself\n",
    "        \n",
    "        # Propagate the gradient back through the hidden layers\n",
    "        dA = np.dot(dZ, self.weights[-1].T)  # Error propagated back to previous layer\n",
    "        for i in range(len(self.weights) - 2, -1, -1):  # Loop through hidden layers\n",
    "            dZ = dA * self.sigmoid_derivative(self.a[i + 1])\n",
    "            dW.append(np.dot(self.a[i].T, dZ)) # * d z_N / d w_N\n",
    "            db.append(np.sum(dZ, axis=0, keepdims=True)) # dZ itself\n",
    "            dA = np.dot(dZ, self.weights[i].T)\n",
    "        return dW[::-1], db[::-1]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        # Mean Squared Error loss function\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def loss_derivative(self, y_pred, y_true):\n",
    "        # Mean Squared Error loss' derivative function\n",
    "        return 2 * (y_pred - y_true) / self.batch_size\n",
    "        \n",
    "    def apply_gradient(self, dW, db):\n",
    "        # Update gradients and biases using the gradient (gradient descent)\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * dW[i]\n",
    "            self.biases[i] -= self.learning_rate * db[i]\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, print_rate = 10):\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            dW, db = self.backward_propagation(X, y)\n",
    "            if (epoch+1) % print_rate == 0:\n",
    "                y_pred = self.forward_propagation(X)\n",
    "                loss = self.compute_loss(y_pred, y)\n",
    "                print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "            self.apply_gradient(dW, db)\n",
    "                \n",
    "    def train_mini_batch(self, X, y, batch_size=32, epoches=1000, print_rate = 10):\n",
    "        m = X.shape[0]\n",
    "        for epoch in tqdm(range(epoches)):\n",
    "            indices = np.random.permutation(m)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            for i in range(0, m, batch_size):  # Repetition in batch_size measure\n",
    "                # Extraction\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                dW, db = self.backward_propagation(X_batch, y_batch)\n",
    "                \n",
    "                if (epoch+1) % print_rate == 0:\n",
    "                    y_pred = self.forward_propagation(X)\n",
    "                    loss = self.compute_loss(y_pred, y)\n",
    "                    print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "                    \n",
    "                self.apply_gradient(dW, db)\n",
    "\n",
    "# Example usage:\n",
    "num_samples = 1000\n",
    "input_size = 3  # Number of input features\n",
    "output_size = 2  # Output layer with 2 neurons\n",
    "\n",
    "hidden_layers = [4, 5]  # Two hidden layers with 4 and 5 neurons\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Instantiate the model\n",
    "model = DNN(input_size, hidden_layers, output_size, learning_rate)\n",
    "\n",
    "# Example data generator\n",
    "def generate_data(num_samples, input_size, output_size):\n",
    "    # generate X (from std norm dist)\n",
    "    X = np.random.randn(num_samples, input_size)\n",
    "\n",
    "    # Generate labeled y (one-hot encoded)\n",
    "    y = np.zeros((num_samples, output_size))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Randomly assigning 0 or 1 in binary classification\n",
    "        label = np.random.randint(0, output_size)\n",
    "        y[i, label] = 1\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Example input data (X) and labels (y)\n",
    "X, y = generate_data(num_samples, input_size, output_size)\n",
    "\n",
    "# Train the model\n",
    "model.train(X, y, epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
