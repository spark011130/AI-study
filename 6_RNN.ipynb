{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN (Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN is a type of network that is great for processing sequences of data. It memorizes the past inputs, which means it shares the past weight to process the current input. The backpropagation method is done through time; a.k.a BPTT (Back Propagation Through Time).\n",
    "\n",
    "It is used in natural language processing (NLP), time series prediction, and speech/audio processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each Element Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./files/RNN/RNN_example.png\" width=\"400\" height=\"250\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./files/RNN/RNN_memo.png\" width=\"600\" height=\"250\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a **Input Layer**, **Hidden State**, and **Output Layer**. \n",
    "\n",
    "Hidden state is calculated as:\n",
    "$$\n",
    "z_{th} = W_{xh}x_t + W_{hh}h_{t-1} + b_h \\\\\n",
    "h_t = tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h) = tanh(z_{th})\n",
    "$$\n",
    "Where $h_t$ is hidden state in time t, $W_{xh}$ is input => current hidden state weight, and $W_{hh}$ is past hidden state => current hidden state weight.\n",
    "\n",
    "Output Layer is calculated as:\n",
    "$$\n",
    "z_{ty} = W_{hy}h_t + b_y \\\\ \n",
    "y_t = softmax(W_{hy}h_t + b_y) = \\sigma (W_{hy}h_t + b_y) = \\sigma (z_{ty})\\\\\n",
    "\n",
    "$$\n",
    "Where $W_{hy}$ is current hidden state => logit weight, and $b_y$ is its bias.\n",
    "\n",
    "As you can see, the input->hidden state weight/bias and hidden state->output weight/bias are not depending on the time and it is shared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the input sequence, get the output predictions. And by the error calculation, we can optimize the weight using back propagation.\n",
    "\n",
    "First, we are going to get the gradients of $h_t, b_y, W_{hy}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let Loss $L = \\sum(y_t - y_t^{true})^2$, where $y_t$ is a output value after the softmax, and $y_t^{true}$ is one-hot encoded true value.\n",
    "$$\n",
    "L = \\sum(y_t - y_t^{true})^2 \\\\\n",
    "\\frac{\\partial L}{\\partial y_t} = 2(y_{t} - y_{t}^{true})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\n",
    "$$ \n",
    "y_t = \\sigma (W_{hy}h_t + b_y) \\\\\n",
    "z_{ty} = W_{hy}h_t + b_y \\\\ \n",
    "\\frac{\\partial y_t}{\\partial z_{ty}} = \\sigma '(z_{ty})\n",
    "$$\n",
    "If so,\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial h_t} &= \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial z_{ty}} \\cdot \\frac{\\partial z_{ty}}{\\partial h_t} \\\\\n",
    "&= 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot W_{hy} \\\\ \n",
    "\n",
    "\\frac{\\partial L}{\\partial b_y} &= \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial z_{ty}} \\cdot \\frac{\\partial z_{ty}}{\\partial b_y}\\\\\n",
    "&= 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot 1 \\\\ \n",
    "\n",
    "\\frac{\\partial L}{\\partial W_{hy}} &= \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial z_{ty}} \\cdot \\frac{\\partial z_{ty}}{\\partial W_{hy}}\\\\\n",
    "&= 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot h_t \\\\ \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to calculate $W_{xh}, W_{hh}, b_h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{align*}\n",
    "\\frac{\\partial L}{\\partial W_{xh}} &= \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial z_{ty}} \\cdot \\frac{\\partial z_{ty}}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial z_{th}} \\cdot \\frac{\\partial z_{th}}{\\partial W_{xh}} \\\\\n",
    "&= 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot W_{hy} \\cdot h_t \\cdot x_t \\\\\n",
    "\n",
    "\\frac{\\partial L}{\\partial W_{hh}} &= \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial z_{ty}} \\cdot \\frac{\\partial z_{ty}}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial z_{th}} \\cdot \\frac{\\partial z_{th}}{\\partial W_{hh}} \\\\\n",
    "&= 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot W_{hy} \\cdot h_t \\cdot h_{t-1} \\\\\n",
    "\n",
    "\\frac{\\partial L}{\\partial b_h} &= \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial z_{ty}} \\cdot \\frac{\\partial z_{ty}}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial z_{th}} \\cdot \\frac{\\partial z_{th}}{\\partial b_h} \\\\\n",
    "&= 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot W_{hy} \\cdot h_t \\cdot 1\n",
    "\n",
    "\\end{align*} $$\n",
    "We could also use this for compact computation:\n",
    "$$ \\begin{align*}\n",
    "\\frac{\\partial L}{\\partial h_t} &= \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial z_{ty}} \\cdot \\frac{\\partial z_{ty}}{\\partial h_t} \\\\\n",
    "&=2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot W_{hy}\n",
    "\\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization\n",
    "It has already been generalized. Just to write down all the gradients together:\n",
    "$$\n",
    "\\begin {align*}\n",
    "\n",
    "\\nabla b_y &= \\frac{\\partial L}{\\partial b_y} = 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot 1 \\\\ \n",
    "\n",
    "\\nabla W_{hy} &= \\frac{\\partial L}{\\partial W_{hy}} = 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot h_t \\\\ \n",
    "\n",
    "\\nabla W_{xh} &= \\frac{\\partial L}{\\partial W_{xh}} = 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot W_{hy} \\cdot h_t \\cdot x_t \\\\\n",
    "\n",
    "\\nabla W_{hh} &= \\frac{\\partial L}{\\partial W_{hh}} = 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot W_{hy} \\cdot h_t \\cdot h_{t-1} \\\\\n",
    "\n",
    "\\nabla b_h &= \\frac{\\partial L}{\\partial b_h} = 2(y_{t} - y_{t}^{true}) \\cdot \\sigma '(z_{ty}) \\cdot W_{hy} \\cdot h_t \\cdot 1 \\\\\n",
    "\\end {align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we don't use softmax function, the term changes:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial z_{ty}} \\cdot \\frac{\\partial z_{ty}}{\\partial h_t} \\\\\n",
    "=> \\frac{\\partial L}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial h_t}\n",
    "$$,\n",
    "\n",
    "So $\\sigma '(z_{ty})$ is removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the sequential $h_t$, we can train the RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't use the batch training, for the samples are not independent, but has a sequential relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 161>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Example data generator\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_data\u001b[39m(num_samples, input_size, output_size):\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# generate X (from std norm dist)\u001b[39;00m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mDNN.__init__\u001b[0;34m(self, input_size, hidden_size, output_size, learning_rate)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m learning_rate\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#Initialize all the weights and biases.\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWxh \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWhh \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(hidden_size, hidden_size) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWhy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(output_size, hidden_size) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:1306\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randn\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:1467\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.standard_normal\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_common.pyx:657\u001b[0m, in \u001b[0;36mnumpy.random._common.cont\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate = 0.01):\n",
    "        '''\n",
    "        Initiate the terms in the RNN according to the parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        input_size (int): size of the input\n",
    "        hidden_size (size): neurons in the hidden layer\n",
    "        output_size (int): size of the output\n",
    "        learning_rate (int): learning rate\n",
    "        \n",
    "        Returns:\n",
    "        self.input_size (int): input size\n",
    "        self.hidden_size (int): hidden layer sizes list\n",
    "        self.output_size (int): output size\n",
    "        self.learning_rate (int): learning rate\n",
    "        self.Wx (np.array): [#hidden size x #input size] x_t to h_t (z_th) weights\n",
    "        self.Wh (np.array): [#hidden size x #hidden size] h_t-1 to h_t weights\n",
    "        self.Wy (np.array): [#output size x #hidden size] h_t to y_t (z_ty) weights\n",
    "        \n",
    "        self.by (np.array): [#hidden size x 1] h_t to y_t (z_ty) biases\n",
    "        self.bh (np.array): [#output size x 1] x_t to h_t (z_th) biases\n",
    "        '''\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        #Initialize all the weights and biases.\n",
    "        self.Wx = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Wy = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        \n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "            \n",
    "    def forward_propagation(self, X):\n",
    "        '''\n",
    "        Forward propagation, X is given as a sequential data.\n",
    "        \n",
    "        parameters:\n",
    "        X (list): t-th sequential data; [input size x 1]\n",
    "        '''\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        z = np.dot(self.Wx, X.reshape(-1, 1)) + np.dot(self.Wh, h) + self.bh\n",
    "        h = np.tanh(z)\n",
    "        output = np.dot(self.Wy, h) + self.by.reshape(-1, 1)\n",
    "        \n",
    "        return output, h\n",
    "    \n",
    "    def backward_propagation(self, X, y):\n",
    "        '''\n",
    "        Backward propagation to get gradients for every weights, and biases.\n",
    "        '''\n",
    "        dWx, dWh, dWy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        \n",
    "        seq_len, _ = X.shape\n",
    "        h = np.zeros((seq_len, self.hidden_size, 1)) # save all time step's hidden states\n",
    "        \n",
    "        # Forward Pass\n",
    "        y_pred = [] # save all time step's prediction\n",
    "        for t in range(seq_len):\n",
    "            y_t, y[t] = self.forward_propagation(X[t])\n",
    "            y_pred.append(y_t)\n",
    "        \n",
    "        # Backward Pass (BPTT)\n",
    "        dh_next = np.zeros_like(h[0]) #initiate the hidden state gradient for next time step\n",
    "        for t in reversed(range(seq_len)):\n",
    "            dy = self.loss_derivative(y_pred[t], y[t])\n",
    "            \n",
    "            dWy += np.dot(dy, h[t].T)\n",
    "            dby += dy\n",
    "            \n",
    "            dh = np.dot(self.Wy.T, dy) + dh_next\n",
    "            dz = dh * (1 - h[t] ** 2) # tanh derivative\n",
    "            \n",
    "            dWx += np.dot(dz, X[t].shape(-1, 1))\n",
    "            dWh += np.dot(dz, h[t - 1].T if t > 0 else np.zeros_like(h[t]))\n",
    "            dbh += dz\n",
    "            \n",
    "            dh_next = np.dot(self.Wh.T, dz)\n",
    "            \n",
    "        return dWx, dWh, dWy, dbh, dby\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        # Mean Squared Error loss function\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def loss_derivative(self, y_pred, y_true):\n",
    "        # Mean Squared Error loss' derivative function\n",
    "        return 2 * (y_pred - y_true) # divided by batch_size; in this case, it is 1.\n",
    "        \n",
    "    def optimizer(self, dWxh, dWhh, dWhy, dbh, dby):\n",
    "        # Update gradients and biases using the gradient (gradient descent)\n",
    "        self.Wxh -= self.learning_rate * dWxh\n",
    "        self.Whh -= self.learning_rate * dWhh\n",
    "        self.Why -= self.learning_rate * dWhy\n",
    "        self.bh -= self.learning_rate * dbh\n",
    "        self.by -= self.learning_rate * dby\n",
    "                \n",
    "    def train_mini_batch(self, X_train, y_train, batch_size=32, epoches=1000, print_rate = 10):\n",
    "        m = X.shape[0]\n",
    "        for epoch in tqdm(range(epoches)):\n",
    "            indices = np.random.permutation(m)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train[indices]\n",
    "            \n",
    "            for i in range(0, m, batch_size):  # Repetition in batch_size measure\n",
    "                # Extraction\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                dWxh, dWhh, dWhy, dbh, dby = self.backward_propagation(X_batch, y_batch)\n",
    "                self.optimizer(dWxh, dWhh, dWhy, dbh, dby)\n",
    "                \n",
    "            if epoch % 10 == 0:\n",
    "                y_pred = self.forward_propagation(X_train)[0]\n",
    "                # training loss calculation\n",
    "                loss = self.compute_loss(y_pred, y_train)\n",
    "                print(f'{epoch}-th epoch MSE loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
